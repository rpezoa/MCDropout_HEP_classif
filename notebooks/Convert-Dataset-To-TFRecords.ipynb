{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f9ae58b",
   "metadata": {},
   "source": [
    "# Uncertainty estimation in deep learning based-classifiers of High Energy Physics events using Monte Carlo Dropout.\n",
    "-----\n",
    "## Higgs Dataset\n",
    "\n",
    "R. Pezoa (UV, CCTVal-USM), S. BÃ³rquez(USM), W. Brooks (USM), L. Salinas (USM), C. Torres (USM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80cdd23",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4e48651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-19 18:22:48.848551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-19 18:22:48.852976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-19 18:22:48.853299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import os\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7e04b94-8e0a-43b5-80d9-4e2053936574",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7120)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d336db4a-c761-453a-99bc-36a9f55f7f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6b018d",
   "metadata": {},
   "source": [
    "## Data\n",
    "-----\n",
    "Hggs dataset\n",
    "\n",
    "- Data is obtained from: https://www.openml.org/d/23512\n",
    "- Each event is represented by a set of 28 features, including 21 low-level features corresponding to physics properties measured by the detector, and 7 high-level features derived from the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7e8fb86-eea7-4c78-b511-8b6ed0a6f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path = \"/mnt/storage-large/dataset/higgs/phpZLgL9q.csv\"\n",
    "data_path = \"/mnt/storage-large/dataset/higgs/HIGGS.csv\"\n",
    "name_columns = [\n",
    "    \"class\",\"lepton_pT\",\"lepton_eta\",\"lepton_phi\",\"missing_energy_magnitude\",\"missing_energy_phi\",\"jet1pt\",\"jet1eta\",\"jet1phi\",\"jet1b-tag\",\"jet2pt\",\"jet2eta\",\"jet2phi\",\"jet2b-tag\",\"jet3pt\",\"jet3eta\",\"jet3phi\",\"jet3b-tag\",\"jet4pt\",\"jet4eta\",\"jet4phi\",\"jet4b-tag\",\"m_jj\",\"m_jjj\",\"m_lv\",\"m_jlv\",\"m_bb\",\"m_wbb\",\"m_wwbb\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e68f2f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_=420\n",
    "# Read data file\n",
    "df = pd.read_csv(data_path, names=name_columns)\n",
    "df.rename(columns = {'class': 'label'}, inplace = True)\n",
    "\n",
    "# Features names\n",
    "y = df[\"label\"]\n",
    "X = df.iloc[:,1:]\n",
    "features_names = list(X.columns)\n",
    "\n",
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(X)\n",
    "\n",
    "del X\n",
    "del df\n",
    "\n",
    "X = scaled_data\n",
    "dataset_indices = np.arange(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08133da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training, validation, and testing data\n",
    "idx_train, idx_test, _, _ = train_test_split(dataset_indices, dataset_indices, shuffle=True, test_size=0.005, random_state=seed_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3e53c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# X_train: 10945000\n",
      "# X_test: 55000\n"
     ]
    }
   ],
   "source": [
    "print(\"# X_train: %s\" % (idx_train.shape[0]))\n",
    "print(\"# X_test: %s\" % (idx_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5785082-1648-41ea-ad52-f87d8e6679ee",
   "metadata": {},
   "source": [
    "## Save dataset in TF records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb4914e7-902f-4007-9b89-ab386e20ed10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(idx):\n",
    "    \"\"\"\n",
    "    Creates a tf.train.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    # Create a dictionary mapping the feature name to the tf.train.Example-compatible\n",
    "    # data type.\n",
    "    idx = idx.numpy()\n",
    "    features = {}\n",
    "    example_X = X[idx]\n",
    "    example_y = y[idx]\n",
    "    features = {\n",
    "        'label': tf.train.Feature(float_list=tf.train.FloatList(value=[example_y]))\n",
    "    }\n",
    "    for i, feature_name in enumerate(features_names):\n",
    "        features[feature_name] = tf.train.Feature(float_list=tf.train.FloatList(value=[example_X[i]]))\n",
    "    # Create a Features message using tf.train.Example.\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "    return example_proto.SerializeToString()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59de1b68-d660-465e-ae71-01bc7c5fd0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-19 18:24:05.222900: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-19 18:24:05.229816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-19 18:24:05.230289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-19 18:24:05.230588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-19 18:24:06.531854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-19 18:24:06.532215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-19 18:24:06.532521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-19 18:24:06.533015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7120 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2022-09-19 18:24:06.541958: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 87560000 exceeds 10% of free system memory.\n",
      "2022-09-19 18:24:06.600757: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 87560000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(idx_train)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1db59c66-616e-4b26-8b2a-82733d56a123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tf_record(dataset, dataset_size, dataset_path, tf_filename, tf_record_size = 500_000):\n",
    "    writer = None\n",
    "    tf_file_id = 0\n",
    "    for n, idx in enumerate(tqdm.tqdm(dataset, total=dataset_size)):\n",
    "        if (n % tf_record_size) == 0:\n",
    "            if writer is not None:\n",
    "                writer.close()\n",
    "            writer = tf.io.TFRecordWriter(\n",
    "                os.path.join(dataset_path, f'{tf_filename}_{tf_file_id:03}.tfrecord')\n",
    "            )\n",
    "            tf_file_id += 1\n",
    "        example = serialize_example(idx)\n",
    "        writer.write(example)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "285b662a-e124-4b92-a142-53ecfa1a867b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                        | 0/10945000 [00:00<?, ?it/s]2022-09-19 18:24:06.637693: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 87560000 exceeds 10% of free system memory.\n",
      "100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 10945000/10945000 [43:04<00:00, 4234.89it/s]\n"
     ]
    }
   ],
   "source": [
    "write_tf_record(\n",
    "    dataset = train_dataset,\n",
    "    dataset_size = len(idx_train),\n",
    "    dataset_path = \"/mnt/storage-large/dataset/higgs/higgs_tfrecords/train\",\n",
    "    tf_filename = 'train',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "752717f7-eb4b-4877-a692-c306acc21f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 55000/55000 [00:13<00:00, 4193.95it/s]\n"
     ]
    }
   ],
   "source": [
    "write_tf_record(\n",
    "    dataset = test_dataset,\n",
    "    dataset_size = len(idx_test),\n",
    "    dataset_path = \"/mnt/storage-large/dataset/higgs/higgs_tfrecords/test\",\n",
    "    tf_filename = 'test',\n",
    "    tf_record_size = 10_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f53820c-2a18-4817-9be6-5d95f3c3078b",
   "metadata": {},
   "source": [
    "## Load tf records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7032884-a39e-4b81-87a0-b779130ef389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asuka/miniconda3/envs/higgs/lib/python3.9/site-packages/tensorflow/python/data/ops/structured_function.py:264: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = tf.io.gfile.glob(\"/mnt/storage-large/dataset/higgs/higgs_tfrecords/test/*.tfrecord\")\n",
    "raw_dataset = tf.data.TFRecordDataset(filenames)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "56fd92ae-754d-4d34-8cf9-c0c7a33afde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "autotune_ = tf.data.AUTOTUNE\n",
    "\n",
    "feature_names = [\n",
    "   'lepton_pT',\n",
    "   'lepton_eta',\n",
    "   'lepton_phi',\n",
    "   'missing_energy_magnitude',\n",
    "   'missing_energy_phi',\n",
    "   'jet1pt',\n",
    "   'jet1eta',\n",
    "   'jet1phi',\n",
    "   'jet1b-tag',\n",
    "   'jet2pt',\n",
    "   'jet2eta',\n",
    "   'jet2phi',\n",
    "   'jet2b-tag',\n",
    "   'jet3pt',\n",
    "   'jet3eta',\n",
    "   'jet3phi',\n",
    "   'jet3b-tag',\n",
    "   'jet4pt',\n",
    "   'jet4eta',\n",
    "   'jet4phi',\n",
    "   'jet4b-tag',\n",
    "   'm_jj',\n",
    "   'm_jjj',\n",
    "   'm_lv',\n",
    "   'm_jlv',\n",
    "   'm_bb',\n",
    "   'm_wbb',\n",
    "   'm_wwbb',\n",
    "]\n",
    "\n",
    "\n",
    "def _parse_function(example_proto, with_label=True):\n",
    "    # Create a description of the features.\n",
    "    feature_description = {\n",
    "        feature_name: tf.io.FixedLenFeature([], tf.float32, default_value=0)\n",
    "        for feature_name in features_names\n",
    "    }\n",
    "    if with_label:\n",
    "        feature_description['label'] = tf.io.FixedLenFeature([], tf.float32, default_value=0)\n",
    "\n",
    "    # Parse the input `tf.train.Example` proto using the dictionary above.\n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    X = [example[feature_name] for feature_name in features_names]\n",
    "    if with_label:\n",
    "        y = example['label']\n",
    "        return X, y\n",
    "    return X,\n",
    "\n",
    "def get_dataset(filenames_template=\"/mnt/storage-large/dataset/higgs/higgs_tfrecords/test/*.tfrecord\", with_label=True, batch_size=32):\n",
    "    filenames = tf.io.gfile.glob(filenames_template)\n",
    "    raw_dataset = tf.data.TFRecordDataset(filenames)\n",
    "    parser = partial(_parse_function, with_label=with_label)\n",
    "    dataset = raw_dataset.map(parser, num_parallel_calls=autotune_)\n",
    "    dataset = dataset.shuffle(seed_)\n",
    "    dataset = dataset.prefetch(buffer_size=autotune_)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2742b0c5-686a-4d68-977a-bb65bc8b9b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0) X.shape=TensorShape([32, 28]), y.shape=TensorShape([32])\n",
      "1) X.shape=TensorShape([32, 28]), y.shape=TensorShape([32])\n"
     ]
    }
   ],
   "source": [
    "parsed_dataset = get_dataset()\n",
    "for i, (X, y) in enumerate(parsed_dataset.take(2)):\n",
    "    print(f'{i}) {X.shape=}, {y.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "56e81399-f1cf-433c-b378-5c17320cb4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0) X.shape=TensorShape([32, 28])\n",
      "1) X.shape=TensorShape([32, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asuka/miniconda3/envs/higgs/lib/python3.9/site-packages/tensorflow/python/data/ops/structured_function.py:264: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "parsed_dataset = get_dataset(with_label=False)\n",
    "for i, (X,) in enumerate(parsed_dataset.take(2)):\n",
    "    print(f'{i}) {X.shape=}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3aecf5715e11a78c2618bdb2316cc3cb85e68efb4ce3d948bdd007a3315b4a57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
